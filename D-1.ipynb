{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# 1. Prediction Task\n",
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1715995592399
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from pprint import pprint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from scipy.stats import ttest_rel\n",
    "from scipy.stats import randint\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from math import sqrt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1715995592666
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DataTrain-2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset1 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDataTrain-2.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(dataset1\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DataTrain-2.csv'"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataset1 = pd.read_csv('DataTrain-2.csv')\n",
    "print(dataset1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995592718
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(dataset1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995592853
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(dataset1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Modeling\n",
    "## 1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995592924
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "max_cycles = dataset1.groupby('engine_id')['cycle'].max().reset_index()\n",
    "max_cycles.rename(columns={'cycle': 'max_cycle'}, inplace=True)\n",
    "dataset1 = dataset1.merge(max_cycles, on='engine_id', how='left')\n",
    "dataset1['RUL'] = dataset1['max_cycle'] - dataset1['cycle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995592983
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "dataset1['RUL'] ,max_cycles, dataset1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_cycles = dataset1.groupby('engine_id')['cycle'].max().reset_index()\n",
    "max_cycles.rename(columns={'cycle': 'max_cycle'}, inplace=True)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(max_cycles['max_cycle'], bins=50, alpha=0.75, color='blue', edgecolor='black')\n",
    "plt.title('Maximum Remaining Useful Life (Max RUL) per Engine')\n",
    "plt.xlabel('Maximum Remaining Useful Life (Max RUL)')\n",
    "plt.ylabel('Count of Engines')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995593765
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "#histogram for each sensor val\n",
    "dataset1[[f'sensor_val{i+1}' for i in range(21)]].hist(bins=50, figsize=(15, 20))\n",
    "plt.suptitle('Distribution of Sensor Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995595196
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# Correlation matrix\n",
    "correlation_matrix = dataset1.corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix of Features and RUL')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation matrix helps us find which features are most linked to RUL, making them useful for predicting outcomes in models. It also helps spot multicollinearity, where high correlations between features can cause problems like overfitting or giving too much weight to certain features in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the correlation of RUL with other features\n",
    "correlation_rul = correlation_matrix['RUL'].sort_values(ascending=False)\n",
    "correlation_rul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995595361
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#scatter plot\n",
    "fig, axs = plt.subplots(7, 3, figsize=(15, 20))\n",
    "axs = axs.flatten()\n",
    "\n",
    "# Plot each sensor against cycle and RUL\n",
    "for i in range(21):\n",
    "    ax = axs[i]\n",
    "    sensor_col = f'sensor_val{i+1}'  # Column name for each sensor\n",
    "    ax.scatter(dataset1['cycle'], dataset1[sensor_col], alpha=0.5, label='Cycle')\n",
    "    ax.scatter(dataset1['RUL'], dataset1[sensor_col], alpha=0.5, label='RUL')\n",
    "    ax.set_xlabel('Cycle / Remaining Useful Life')\n",
    "    ax.set_ylabel(f'Sensor Value {i+1}')\n",
    "    ax.set_title(f'Sensor Value {i+1} across Cycles and RUL')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this visualisation, the scatter plot is used to visually explore how Sensor Values behaves concerning 'cycle' and 'RUL' in the dataset. It is essential for quickly understanding relationships between variables like sensor readings and time-related metrics such as 'Cycle' or 'RUL'. It help identify patterns, outliers, and correlations visually, aiding in feature selection for predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values in the dataset\n",
    "dataset1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a feature  for the remaining cycles for each cycle based on the max cycle number for each engine id in dataset\n",
    "dataset1['RUL'] = dataset1['max_cycle'] - dataset1['cycle']\n",
    "dataset1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995595420
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Dropping colun not useful fro training\n",
    "features = dataset1.drop(columns=['RUL', 'engine_id', 'max_cycle' ])\n",
    "target = dataset1['RUL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.shape, features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Training Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995595512
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "X_test.shape, X_train.shape, y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995611763
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# base model\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=20, min_samples_split=5, min_samples_leaf=2,random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "# Predict RUL on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred\n",
    "\n",
    "#Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate the root mean squared error\n",
    "rmse_scorer = make_scorer(lambda y_true, y_pred: sqrt(mean_squared_error(y_true, y_pred)), greater_is_better=False)\n",
    "\n",
    "# Perform cross-validation\n",
    "scores = cross_val_score(model, X_train, y_train, cv=KFold(n_splits=5, shuffle=True, random_state=42), scoring=rmse_scorer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output \n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean RMSE:\", scores.mean())\n",
    "print(\"Standard deviation:\", scores.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1715995611870
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "importances = model.feature_importances_\n",
    "# CreateDataFrame to store feature importances\n",
    "feature_importance_df = pd.DataFrame({'Feature': X_train.columns, 'Importance': importances})\n",
    "# Sort DataFrame by importance: descending order\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter 0 importance feature for next models\n",
    "\n",
    "features_to_keep = feature_importance_df[feature_importance_df['Importance'] > 0]['Feature']\n",
    "X_train = X_train[features_to_keep]\n",
    "X_test = X_test[features_to_keep]\n",
    "\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [70, 80, 90, 100],\n",
    "    'ccp_alpha': [0],\n",
    "    'max_leaf_nodes': [None],\n",
    "    'max_samples': [None],\n",
    "    'max_features': [1],\n",
    "    'max_depth': [None],\n",
    "    'random_state': [18],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best parameters from Grid Search:\", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model with best parameters from grid search\n",
    "model = grid_search.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Grid Search - MAE:\", mae, \"RMSE:\", rmse, \"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define randomized search\n",
    "param_dist = {\n",
    "    'n_estimators': randint(70, 100),\n",
    "    'ccp_alpha': [0],\n",
    "    'max_leaf_nodes': [None],\n",
    "    'max_samples': [None],\n",
    "    'max_features': [1],\n",
    "    'max_depth': [None],\n",
    "    'random_state': [18],\n",
    "    'min_samples_split': randint(2, 10),\n",
    "    'min_samples_leaf': randint(1, 4),\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=20, cv=5, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"Best parameters from Randomized Search:\", random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model with best parameters from randomized search\n",
    "model = random_search.best_estimator_\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Randomized Search - MAE:\", mae, \"RMSE:\", rmse, \"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup RMSE scorer for cross-validation\n",
    "def rmse_scorer(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Cross-validation using the final model\n",
    "scores = cross_val_score(model, X_train, y_train, cv=KFold(n_splits=5, shuffle=True, random_state=42), scoring=make_scorer(rmse_scorer, greater_is_better=False))\n",
    "print(\"Cross-validation scores:\", scores)\n",
    "print(\"Mean RMSE:\", scores.mean())\n",
    "print(\"Standard deviation:\", scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_from_grid_search = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataset2 = pd.read_csv('DataSchedule-2.csv')\n",
    "dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `original_features` is a list of feature names used during model training\n",
    "original_features = X_train.columns.tolist()\n",
    "\n",
    "# Check for any missing features and fill with zero or appropriate values\n",
    "for feature in original_features:\n",
    "    if feature not in dataset2.columns:\n",
    "        dataset2[feature] = 0  # Or use another placeholder value like the mean of the column\n",
    "\n",
    "# Remove any extra features not used during training\n",
    "drop_eid = dataset2[original_features]\n",
    "\n",
    "# Predict using the best model from grid search\n",
    "pred_rul = best_model_from_grid_search.predict(drop_eid)\n",
    "pred_rul = pred_rul.astype(int)\n",
    "\n",
    "# Adding the predictions back to the original dataset\n",
    "dataset2['RUL'] = pred_rul\n",
    "\n",
    "# Optionally, if you need to keep the 'engine_id' for reference\n",
    "dataset2['engine_id'] = dataset2['engine_id']\n",
    "\n",
    "print(dataset2[['engine_id', 'RUL']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3 = pd.read_csv('RUL_consultancy_predictions_A3-2.csv')\n",
    "dataset3\n",
    "# delimeter ;\n",
    "dataset3 = pd.read_csv('RUL_consultancy_predictions_A3-2.csv', delimiter=';')\n",
    "dataset3\n",
    "# Convert 'RUL' and 'id' columns to appropriate data types (integers)\n",
    "dataset3['RUL'] = dataset3['RUL'].astype(int)\n",
    "dataset3['id'] = dataset3['id'].astype(int)\n",
    "dataset3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Average to calculate single RUL\n",
    "# Function to compute weighted average for each engine_id\n",
    "def compute_weighted_avg(group):\n",
    "    return group['weighted_rul'].sum() / group['weight'].sum()\n",
    "\n",
    "weighted_rul_df = dataset2.copy()\n",
    "weighted_rul_df['weight'] = weighted_rul_df['cycle'] / weighted_rul_df.groupby('engine_id')['cycle'].transform('max')\n",
    "weighted_rul_df['weighted_rul'] = weighted_rul_df['RUL'] * weighted_rul_df['weight']\n",
    "weighted_avg_rul = weighted_rul_df.groupby('engine_id').apply(compute_weighted_avg).reset_index()\n",
    "weighted_avg_rul.columns = ['id', 'RUL']\n",
    "weighted_avg_rul['RUL'] = weighted_avg_rul['RUL'].round().astype(int)\n",
    "merged_results = pd.merge(weighted_avg_rul, dataset3, on='id', how='inner', suffixes=('_predicted', '_consultancy'))\n",
    "\n",
    "predictions_df = merged_results.drop(columns=['RUL_consultancy'])\n",
    "print(merged_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the RUL columns\n",
    "rul_predicted = merged_results['RUL_predicted']\n",
    "rul_consultancy = merged_results['RUL_consultancy']\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(rul_consultancy, rul_predicted)\n",
    "rmse = mean_squared_error(rul_consultancy, rul_predicted, squared=False)\n",
    "mse = mean_squared_error(rul_consultancy, rul_predicted)\n",
    "\n",
    "# Output\n",
    "print(\"Mean Absolute Error:\", mae)\n",
    "print(\"Root Mean Squared Error:\", rmse)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform statistical tests paired with t-test\n",
    "t_stat, p_value = ttest_rel(rul_consultancy, rul_predicted)\n",
    "\n",
    "# Output\n",
    "print(\"T-statistic:\", t_stat)\n",
    "print(\"P-value:\", p_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save prediction to a CSV file\n",
    "predictions_df.to_csv('rul_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Optimization Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Genetic Algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from deap import base, creator, tools, algorithms\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read lr_predictions.csv and copy it to a new DataFrame\n",
    "predictions = pd.read_csv('lr_predictions.csv')\n",
    "predictions_copy = predictions.copy()\n",
    "predictions_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this task, the goal is to develop a maintenance schedule for the engines in data set 2. The output of Task 1.2 should be a list that contains a predicted RUL for each engine in data set 2. These predicted values will now be used as input in order to allocate teams of workers to perform maintenance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to allocate workers to different engines in order to perform maintenance. Workers perform maintenance in teams, and teams can only work on one engine at a time. Also, two teams cannot work on the same engine simultaneously. There are different types of teams: teams of type A and teams of type B. There are G teams in total. In our case, assume that there are G = 4 teams: 2 teams of type A and 2 teams of type B. Assume that teams T1,T3 are type A, and teams T2,T4 are type B. The engines that need maintenance are numbered with indices from the set M= {1,...,M}, with M being the total number of engines. Teams of type A need μAj days to perform maintenance on engine j ∈ M, and teams of type B need μBj days. Teams of type A are more efficient, as μAj < μBj ∀j ∈M. In our case, M = 100. The values for μAj ,j ∈ M are as follows: μAj = 4,j ∈ {1,2,...,20}, (ii) μAj = 3,j ∈ {21,22,...,55}, (iii) μAj = 2,j ∈ {56,57,...,80}, and (iv) μAj = 8,j ∈ {81,82,...,100}. The values for μBj ,j ∈ M are as follows: μBj = μAj + 1,j ∈ {1,2,...,25}, (ii) μBj = μAj + 2,j ∈ {26,27,...,70}, and (iii) μBj = μAj + 1,j ∈{71,72,...,100}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the total number of engines\n",
    "M = 100\n",
    "# Define the number of teams\n",
    "G = 4\n",
    "# Define the time horizon\n",
    "T = 30\n",
    "# Define the maximum daily penalty cost\n",
    "MAX_DAILY_COST = 250\n",
    "\n",
    "\n",
    "\n",
    "# Initialize lists to store μAj and μBj values for each engine\n",
    "mu_A = [0] * M # mu_A is maintenance time for team A\n",
    "mu_B = [0] * M # mu_B is maintenance time for team B\n",
    "engine_costs = []\n",
    "\n",
    "# Populate engine_costs with values according to the given rules\n",
    "for j in range(1, M + 1):\n",
    "    if j <= 20:\n",
    "        engine_costs.append(4)\n",
    "    elif 21 <= j < 31:\n",
    "        engine_costs.append(3)\n",
    "    elif 31 <= j < 46:\n",
    "        engine_costs.append(2)\n",
    "    elif 46 <= j < 81:\n",
    "        engine_costs.append(5)\n",
    "    else:\n",
    "        engine_costs.append(6)\n",
    "\n",
    "# Populate mu_A with values according to the given rules\n",
    "for j in range(1, M + 1):\n",
    "    if 1 <= j <= 20:\n",
    "        mu_A[j-1] = 4\n",
    "    elif 21 <= j <= 55:\n",
    "        mu_A[j-1] = 3\n",
    "    elif 56 <= j <= 80:\n",
    "        mu_A[j-1] = 2\n",
    "    elif 81 <= j <= 100:\n",
    "        mu_A[j-1] = 8\n",
    "\n",
    "# Populate mu_B based on mu_A values and the given rules\n",
    "for j in range(1, M + 1):\n",
    "    if 1 <= j <= 25:\n",
    "        mu_B[j-1] = mu_A[j-1] + 1\n",
    "    elif 26 <= j <= 70:\n",
    "        mu_B[j-1] = mu_A[j-1] + 2\n",
    "    elif 71 <= j <= 100:\n",
    "        mu_B[j-1] = mu_A[j-1] + 1\n",
    "\n",
    "# Print results\n",
    "print(\"Engine Costs:\", engine_costs)\n",
    "print(\"μA values:\", mu_A)\n",
    "print(\"μB values:\", mu_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of teams of type A and type B\n",
    "G_A = 2\n",
    "G_B = 2\n",
    "\n",
    "# Define the teams of type A and type B\n",
    "teams_A = ['T1', 'T3']\n",
    "teams_B = ['T2', 'T4']\n",
    "\n",
    "# Define the total number of teams\n",
    "teams = teams_A + teams_B\n",
    "\n",
    "# Output the teams\n",
    "print(\"Teams of type A:\", teams_A)\n",
    "print(\"Teams of type B:\", teams_B)\n",
    "print(\"All teams:\", teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Genetic Algorithm - First, you are going to develop a Genetic Algorithm to solve the optimization problem. Company X is going to allocate teams to different engines in order to perform maintenance. Assume that we are currently at day t = 1 and that company X wants to allocate teams to engines in order to minimize penalty costs for a planning horizon of T = 30. In other words, company X wants to allocate teams to engines that have a predicted safety due date of less than 30.\n",
    "# 2.1.1 Write a Genetic Algorithm that solves the problem for company X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the creator for the fitness and individual classes\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(-1.0,))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# Function to create a feasible individual\n",
    "def create_individual():\n",
    "    individual = []\n",
    "    for engine_id in range(1, len(predictions_copy) + 1):\n",
    "        if predictions_copy.loc[predictions_copy['engine_id'] == engine_id, 'RUL'].values[0] < T:\n",
    "            team_type = random.choice(['A', 'B'])\n",
    "            start_day = random.randint(1, T)\n",
    "            individual.append((engine_id, team_type, start_day))\n",
    "    return individual\n",
    "\n",
    "# Function to convert an individual to a schedule and return DataFrame\n",
    "def individual_to_schedule(individual):\n",
    "    penalty_cost = 0\n",
    "    schedule = []\n",
    "    total_penalty_cost = 0\n",
    "    for engine_id, team_type, start_day in individual:\n",
    "        predicted_rul = predictions_copy.loc[predictions_copy['engine_id'] == engine_id, 'RUL'].values[0]\n",
    "        if team_type == 'A':\n",
    "            maintenance_time = mu_A[engine_id - 1]\n",
    "        else:\n",
    "            maintenance_time = mu_B[engine_id - 1]\n",
    "        end_day = start_day + maintenance_time - 1\n",
    "        if end_day > predicted_rul:\n",
    "            penalty_cost = (end_day - predicted_rul) * MAX_DAILY_COST\n",
    "            total_penalty_cost += penalty_cost\n",
    "        schedule.append({'RUL': predicted_rul, 'Engine_id': engine_id, 'Team': team_type, 'Start_date': start_day, 'End_date': end_day, 'Penalty_cost': penalty_cost, 'Total_penalty_cost': total_penalty_cost})\n",
    "    return pd.DataFrame(schedule), total_penalty_cost\n",
    "    \n",
    "# Function to calculate penalty cost per engine based on maintenance schedule\n",
    "def calculate_penalty_cost(engine_id, team_type, start_day):\n",
    "    predicted_rul = predictions_copy.loc[predictions_copy['engine_id'] == engine_id, 'RUL'].values[0]\n",
    "    if team_type == 'A':\n",
    "        maintenance_time = mu_A[engine_id - 1]\n",
    "    else:\n",
    "        maintenance_time = mu_B[engine_id - 1]\n",
    "    end_day = start_day + maintenance_time - 1\n",
    "    penalty = 0\n",
    "\n",
    "    if end_day > predicted_rul:\n",
    "        overdue_days = end_day - predicted_rul\n",
    "        penalty = min(MAX_DAILY_COST, engine_costs[engine_id - 1] * overdue_days ** 2)\n",
    "\n",
    "    return penalty\n",
    "\n",
    "# Fitness function: total penalty cost for the individuaL\n",
    "def calculate_fitness(individual):\n",
    "    total_penalty_cost = 0\n",
    "    for engine_id, team_type, start_day in individual:\n",
    "        penalty_cost = calculate_penalty_cost(engine_id, team_type, start_day)\n",
    "        total_penalty_cost += penalty_cost\n",
    "    return (total_penalty_cost,)\n",
    "\n",
    "# Feasibility function: check if the individual satisfies all constraints\n",
    "def is_feasible(individual):\n",
    "    engine_ids = [engine_id for engine_id, _, _ in individual]\n",
    "    return len(engine_ids) == len(set(engine_ids))\n",
    "\n",
    "def cxTwoPoint(ind1, ind2):\n",
    "    size = min(len(ind1), len(ind2))\n",
    "    print(f\"Individual lengths: {len(ind1)}, {len(ind2)}\")\n",
    "    print(f\"Size: {size}\")\n",
    "    \n",
    "    cxpoint1 = random.randint(1, size)\n",
    "    cxpoint2 = random.randint(1, size - 1)\n",
    "    print(f\"Crossover points: {cxpoint1}, {cxpoint2}\")\n",
    "    \n",
    "    if cxpoint2 >= cxpoint1:\n",
    "        ind1[cxpoint1:cxpoint2], ind2[cxpoint1:cxpoint2] = ind2[cxpoint1:cxpoint2], ind1[cxpoint1:cxpoint2]\n",
    "\n",
    "    return ind1, ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the toolbox\n",
    "toolbox = base.Toolbox()\n",
    "toolbox.register(\"individual\", tools.initIterate, creator.Individual, create_individual)\n",
    "toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "toolbox.register(\"evaluate\", calculate_fitness)\n",
    "toolbox.decorate(\"evaluate\", tools.DeltaPenalty(is_feasible, 10000))\n",
    "toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "toolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.1)\n",
    "toolbox.register(\"select\", tools.selTournament, tournsize=2)\n",
    "\n",
    "\n",
    "# Create intial population\n",
    "population = toolbox.population(n=50)\n",
    "# Define statistics and hall of fame\n",
    "stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "stats.register(\"avg\", np.mean, axis=0)\n",
    "stats.register(\"std\", np.std, axis=0)\n",
    "stats.register(\"min\", np.min, axis=0)\n",
    "stats.register(\"max\", np.max, axis=0)\n",
    "hof = tools.HallOfFame(1)\n",
    "\n",
    "# Number of generations\n",
    "num_generations = 250\n",
    "\n",
    "# Run the genetic algorithm§\n",
    "population, logbook = algorithms.eaSimple(population, toolbox, cxpb=0.5, mutpb=0.1, ngen=num_generations, stats=stats, halloffame=hof)\n",
    "\n",
    "# Print best individual and its fitness values\n",
    "best_individual = hof[0]\n",
    "best_fitness = best_individual.fitness.values\n",
    "print(\"Best individual is:\", best_individual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# historical data\n",
    "gen = logbook.select(\"gen\")\n",
    "max_val = logbook.select(\"max\")\n",
    "avg_val = logbook.select(\"avg\")\n",
    "min_val = logbook.select(\"min\")\n",
    "\n",
    "# Calculate the coefficients of the linear regression\n",
    "slope, intercept = np.polyfit(gen, avg_val, 1)\n",
    "# Generate y-values based on the slope and intercept\n",
    "trendline = np.polyval([slope, intercept], gen)\n",
    "\n",
    "# Plot the historical data\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(gen, max_val, label='Max Fitness')\n",
    "plt.plot(gen, avg_val, label='Average Fitness')\n",
    "plt.plot(gen, min_val, label='Min Fitness')\n",
    "\n",
    "# Add a linear regression line for the average fitness\n",
    "plt.plot(gen, trendline, color='r', linestyle='-', label='Average Fitness Trend')\n",
    "\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Fitness')\n",
    "plt.title('Genetic Algorithm Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_ga(t, toolbox, T, num_generations):\n",
    "    start_time = time.time()\n",
    "    elapsed_time = 0\n",
    "    best_individuals = []\n",
    "    best_fitness_values = []\n",
    "\n",
    "    while elapsed_time < t:  # 5 minutes\n",
    "        pop = [toolbox.individual() for _ in range(1000)]  # Initialize population directly\n",
    "        hof = tools.ParetoFront()\n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register(\"avg\", np.mean, axis=0)\n",
    "        stats.register(\"std\", np.std, axis=0)\n",
    "        stats.register(\"min\", np.min, axis=0)\n",
    "        stats.register(\"max\", np.max, axis=0)\n",
    "        \n",
    "        pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=0.6, mutpb=0.1,\n",
    "                                   ngen=num_generations, stats=stats, halloffame=hof)\n",
    "        \n",
    "        if hof:\n",
    "            best_individuals.append(hof[0])\n",
    "            best_fitness_values.append(hof[0].fitness.values[0])\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Select the best of the best individuals\n",
    "    best_index = np.argmin(best_fitness_values)\n",
    "    best_individual = best_individuals[best_index]\n",
    "    best_fitness = best_fitness_values[best_index]\n",
    "\n",
    "    return best_individual, best_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_in_seconds = 300 # 5 minutes\n",
    "best_individual, best_fitness = run_ga(t_in_seconds, toolbox, T, num_generations)\n",
    "\n",
    "print(f\"\\nBest individual: {best_individual}\\nFitness: {best_fitness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_individual = hof[0] \n",
    "schedule_df, total_penalty_cost = individual_to_schedule(best_individual)\n",
    "\n",
    "# Display the schedule DataFrame\n",
    "print(\"Schedule:\")\n",
    "print(schedule_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from deap import tools, algorithms\n",
    "\n",
    "def run_ga_multiple_times(t, toolbox, num_generations):\n",
    "    start_time = time.time()\n",
    "    elapsed_time = 0\n",
    "    best_fitness_values = []\n",
    "\n",
    "    while elapsed_time < t:  # 5 minutes\n",
    "        pop = [toolbox.individual() for _ in range(1000)]  # Initialize population directly\n",
    "        hof = tools.ParetoFront()\n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register(\"avg\", np.mean, axis=0)\n",
    "        stats.register(\"std\", np.std, axis=0)\n",
    "        stats.register(\"min\", np.min, axis=0)\n",
    "        stats.register(\"max\", np.max, axis=0)\n",
    "        \n",
    "        pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=0.6, mutpb=0.1,\n",
    "                                   ngen=num_generations, stats=stats, halloffame=hof)\n",
    "        \n",
    "        if hof:\n",
    "            current_best_fitness = hof[0].fitness.values[0]\n",
    "            best_fitness_values.append(current_best_fitness)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Calculate the average best fitness\n",
    "    average_best_fitness = np.mean(best_fitness_values)\n",
    "\n",
    "    return average_best_fitness\n",
    "\n",
    "# Call the function to run the genetic algorithm 30 times for at most 5 minutes\n",
    "average_best_fitness = run_ga_multiple_times(300, toolbox, num_generations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the schedule DataFrame and total penalty costs\n",
    "print(\"Schedule:\")\n",
    "print(schedule_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3 Comparison \n",
    "# Run the Genetic Algorithm on the same problem, but with the RUL predictions of the consultancy company. Run it 30 times for at most 5 minutes, and plot the average-best-fitness.\n",
    "# Create a new DataFrame with the consultancy RUL predictions\n",
    "rul_df = pd.read_csv('RUL_consultancy_predictions_A3-2.csv', delimiter=';')\n",
    "rul_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ga 30 times for 5 minutes and plot the average-fitness-values.\n",
    "def run_ga_multiple_times_consultancy(t, toolbox, num_generations):\n",
    "    start_time = time.time()\n",
    "    elapsed_time = 0\n",
    "    best_fitness_values = []\n",
    "\n",
    "    while elapsed_time < t:  # 5 minutes\n",
    "        pop = [toolbox.individual() for _ in range(1000)]  # Initialize population directly\n",
    "        hof = tools.ParetoFront()\n",
    "        stats = tools.Statistics(lambda ind: ind.fitness.values)\n",
    "        stats.register(\"avg\", np.mean, axis=0)\n",
    "        stats.register(\"std\", np.std, axis=0)\n",
    "        stats.register(\"min\", np.min, axis=0)\n",
    "        stats.register(\"max\", np.max, axis=0)\n",
    "        \n",
    "        pop, logbook = algorithms.eaSimple(pop, toolbox, cxpb=0.6, mutpb=0.1,\n",
    "                                   ngen=num_generations, stats=stats, halloffame=hof)\n",
    "        \n",
    "        if hof:\n",
    "            current_best_fitness = hof[0].fitness.values[0]\n",
    "            best_fitness_values.append(current_best_fitness)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Calculate the average best fitness\n",
    "    average_best_fitness = np.mean(best_fitness_values)\n",
    "\n",
    "    return average_best_fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#priunt the best individual\n",
    "best_individual2, best_fitness = run_ga(t_in_seconds, toolbox, T, num_generations)\n",
    "# Display the schedule DataFrame\n",
    "schedule_df = individual_to_schedule(best_individual2)\n",
    "print(\"Schedule:\")\n",
    "print(schedule_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the final average best fitness\n",
    "average_best_fitness_consultancy = run_ga_multiple_times_consultancy(300, toolbox, num_generations)\n",
    "print(f\"Average Best Fitness: {average_best_fitness_consultancy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the average best fitness values for the two cases\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot([average_best_fitness] * 30, label='Company X')\n",
    "plt.plot([average_best_fitness_consultancy] * 30, label='Consultancy')\n",
    "plt.xlabel('Run')\n",
    "plt.ylabel('Average Best Fitness')\n",
    "plt.title('Average Best Fitness Values for Company X and Consultancy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python310-sdkv2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
